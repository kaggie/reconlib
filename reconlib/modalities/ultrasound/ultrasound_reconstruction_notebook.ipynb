{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Ultrasound Reconstruction Example\n",
        "This notebook demonstrates B-mode like ultrasound image reconstruction using a more realistic forward model and Total Variation (TV) regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure reconlib is in the Python path (adjust if notebook is moved)\n",
        "if 'reconlib' not in os.getcwd():\n",
        "    # Assuming notebook is in reconlib/modalities/ultrasound/\n",
        "    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname('__file__'), '../../..')))\n",
        "else:\n",
        "    # If notebook is in root/examples or similar, and reconlib is a sibling dir\n",
        "    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
        "\n",
        "from reconlib.modalities.ultrasound.operators import UltrasoundForwardOperator\n",
        "from reconlib.modalities.ultrasound.regularizers import UltrasoundTVCustomRegularizer\n",
        "from reconlib.reconstructors.proximal_gradient_reconstructor import ProximalGradientReconstructor\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Simulation Parameters\n",
        "Parameters are chosen to align with typical ultrasound imaging scenarios and the user's pseudocode suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image and Grid Parameters\n",
        "N = 128  # Image size (N x N pixels)\n",
        "pixel_size_m = 0.0002  # Pixel size (0.2 mm)\n",
        "image_shape = (N, N)\n",
        "image_spacing_m = (pixel_size_m, pixel_size_m)\n",
        "img_depth_m = N * pixel_size_m\n",
        "img_width_m = N * pixel_size_m\n",
        "\n",
        "# Ultrasound Physics Parameters\n",
        "sound_speed_mps = 1540.0  # Speed of sound in m/s\n",
        "center_frequency_hz = 5e6  # Center frequency (5 MHz)\n",
        "pulse_bandwidth_fractional = 0.6 # Fractional bandwidth (e.g., 60% of center_freq)\n",
        "sampling_rate_hz = 4 * center_frequency_hz # Sampling frequency (e.g., 20 MHz for 5MHz pulse, Nyquist is 2*f_max)\n",
        "\n",
        "# Transducer Parameters\n",
        "num_elements = 64\n",
        "element_pitch_m = 0.0003  # Element pitch (0.3 mm)\n",
        "array_width_m = (num_elements - 1) * element_pitch\n",
        "element_x_coords = torch.linspace(-array_width_m / 2, array_width_m / 2, num_elements, device=device)\n",
        "# Position elements slightly above the image region (e.g., at y = -2mm relative to image top)\n",
        "element_y_pos_m = -0.002 \n",
        "element_positions = torch.stack(\n",
        "    (element_x_coords, torch.full_like(element_x_coords, element_y_pos_m)), dim=1\n",
        ")\n",
        "beam_sigma_rad = 0.05  # Beam width (radians) - adjust for desired focus/spread\n",
        "\n",
        "# Attenuation\n",
        "attenuation_coeff_db_cm_mhz = 0.5 # Typical for soft tissue\n",
        "\n",
        "# RF Data Simulation Parameters\n",
        "# Number of time samples for RF data: needs to cover round trip to max depth\n",
        "max_time_s = 2 * img_depth_m / sound_speed_mps * 1.2 # Add 20% margin\n",
        "num_samples_rf = int(np.ceil(max_time_s * sampling_rate_hz))\n",
        "print(f\"Calculated num_samples_rf: {num_samples_rf}\")\n",
        "\n",
        "# Reconstruction Parameters\n",
        "lambda_tv_overall = 0.005 # Regularization strength for TV\n",
        "pg_iterations = 50\n",
        "pg_step_size = 0.01 # Initial step size for ProximalGradientReconstructor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Phantom Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_ultrasound_phantom(shape, spacing_m, device='cpu'):\n",
        "    phantom = torch.zeros(shape, dtype=torch.float32, device=device)\n",
        "    h, w = shape\n",
        "    h_m, w_m = h * spacing_m[0], w * spacing_m[1]\n",
        "    \n",
        "    # Central circular inclusion\n",
        "    center_y, center_x = h_m / 2, w_m / 2\n",
        "    radius1 = min(h_m, w_m) / 4\n",
        "    Y, X = torch.meshgrid(torch.linspace(0, h_m, h, device=device), \n",
        "                          torch.linspace(0, w_m, w, device=device), indexing='ij')\n",
        "    mask1 = (X - center_x)**2 + (Y - center_y)**2 < radius1**2\n",
        "    phantom[mask1] = 1.0\n",
        "    \n",
        "    # Smaller, off-center inclusion (hypoechoic - lower reflectivity)\n",
        "    center_y2, center_x2 = h_m * 0.25, w_m * 0.75\n",
        "    radius2 = min(h_m, w_m) / 8\n",
        "    mask2 = (X - center_x2)**2 + (Y - center_y2)**2 < radius2**2\n",
        "    phantom[mask2] = 0.3\n",
        "    \n",
        "    # Point scatterers\n",
        "    phantom[int(h*0.7), int(w*0.3)] = 1.5\n",
        "    phantom[int(h*0.6), int(w*0.6)] = 1.2\n",
        "    \n",
        "    # Smooth slightly to avoid harsh edges (optional)\n",
        "    # phantom = torch_gaussian_filter_2d(phantom.unsqueeze(0).unsqueeze(0), kernel_size=3, sigma=0.5).squeeze()\n",
        "    return phantom.to(torch.complex64) # Operator expects complex\n",
        "\n",
        "phantom_image = generate_ultrasound_phantom(image_shape, image_spacing_m, device=device)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(torch.abs(phantom_image).cpu().numpy(), cmap='gray')\n",
        "plt.title('Original Phantom (Magnitude)')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Ultrasound Forward Operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "us_operator = UltrasoundForwardOperator(\n",
        "    image_shape=image_shape,\n",
        "    sound_speed=sound_speed_mps,\n",
        "    num_elements=num_elements,\n",
        "    element_positions=element_positions,\n",
        "    sampling_rate=sampling_rate_hz,\n",
        "    num_samples=num_samples_rf,\n",
        "    image_spacing=image_spacing_m,\n",
        "    center_frequency=center_frequency_hz,\n",
        "    pulse_bandwidth_fractional=pulse_bandwidth_fractional,\n",
        "    beam_sigma_rad=beam_sigma_rad,\n",
        "    attenuation_coeff_db_cm_mhz=attenuation_coeff_db_cm_mhz,\n",
        "    device=device\n",
        ")\n",
        "print(\"UltrasoundForwardOperator initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Simulate RF Data (Forward Projection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Simulating RF data... This might take a moment.\")\n",
        "rf_data_clean = us_operator.op(phantom_image)\n",
        "print(f\"Simulated clean RF data shape: {rf_data_clean.shape}\")\n",
        "\n",
        "# Add noise\n",
        "signal_power = torch.mean(torch.abs(rf_data_clean)**2)\n",
        "noise_power_ratio = 0.05 # e.g., 5% noise relative to signal power\n",
        "noise_std = torch.sqrt(signal_power * noise_power_ratio / 2) # Factor of 2 for complex noise (real+imag)\n",
        "noise = noise_std * (torch.randn_like(rf_data_clean.real) + 1j * torch.randn_like(rf_data_clean.imag))\n",
        "rf_data_noisy = rf_data_clean + noise\n",
        "print(f\"Added complex Gaussian noise. Noise STD: {noise_std.item()}\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(torch.abs(rf_data_clean).cpu().numpy(), aspect='auto', cmap='viridis')\n",
        "plt.title('Clean RF Data (Magnitude)')\n",
        "plt.xlabel('Time Samples')\n",
        "plt.ylabel('Transducer Element')\n",
        "plt.colorbar()\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(torch.abs(rf_data_noisy).cpu().numpy(), aspect='auto', cmap='viridis')\n",
        "plt.title('Noisy RF Data (Magnitude)')\n",
        "plt.xlabel('Time Samples')\n",
        "plt.ylabel('Transducer Element')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Image Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Adjoint Reconstruction (Delay-and-Sum like)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Performing Adjoint (DAS-like) reconstruction...\")\n",
        "adjoint_recon = us_operator.op_adj(rf_data_noisy)\n",
        "print(f\"Adjoint reconstructed image shape: {adjoint_recon.shape}\")\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(torch.abs(adjoint_recon).cpu().numpy(), cmap='gray')\n",
        "plt.title('Adjoint (DAS-like) Reconstruction')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 TV Regularized Reconstruction using Proximal Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Performing TV Regularized Reconstruction (lambda_TV={lambda_tv_overall})...This may take time.\")\n",
        "tv_regularizer = UltrasoundTVCustomRegularizer(\n",
        "    lambda_reg=lambda_tv_overall, \n",
        "    prox_iterations=10, # Iterations for the inner prox_tv loop\n",
        "    is_3d=False,\n",
        "    prox_step_size=0.01 # Step size for prox_tv's own gradient steps\n",
        ")\n",
        "\n",
        "# ProximalGradientReconstructor setup\n",
        "pg_reconstructor_tv = ProximalGradientReconstructor(\n",
        "    iterations=pg_iterations,\n",
        "    step_size=pg_step_size,\n",
        "    verbose=True,\n",
        "    log_fn=lambda iter_num, current_image, change, grad_norm: \n",
        "        print(f\"Iter {iter_num+1}: Change={change:.2e}, GradNorm={grad_norm:.2e}\") if (iter_num % 10 ==0 or iter_num == pg_iterations -1) else None\n",
        ")\n",
        "\n",
        "# Initial estimate for reconstruction (can be adjoint or zeros)\n",
        "initial_estimate_tv = adjoint_recon.clone() # Start from adjoint\n",
        "# initial_estimate_tv = torch.zeros_like(phantom_image, device=device) # Start from zeros\n",
        "\n",
        "tv_recon_image = pg_reconstructor_tv.reconstruct(\n",
        "    kspace_data=rf_data_noisy, # Our 'y' is the RF data\n",
        "    forward_op_fn=lambda img, smaps: us_operator.op(img), # smaps not used by US op\n",
        "    adjoint_op_fn=lambda data, smaps: us_operator.op_adj(data), # smaps not used\n",
        "    regularizer_prox_fn=lambda img, step: tv_regularizer.proximal_operator(img, step),\n",
        "    sensitivity_maps=None, # No coil sensitivities in this US model\n",
        "    x_init=initial_estimate_tv\n",
        ")\n",
        "print(f\"TV Reconstructed image shape: {tv_recon_image.shape}\")\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(torch.abs(tv_recon_image).cpu().numpy(), cmap='gray')\n",
        "plt.title(f'TV Regularized Reconstruction (lambda={lambda_tv_overall}, {pg_iterations} iters)')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparison of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "axes[0].imshow(torch.abs(phantom_image).cpu().numpy(), cmap='gray')\n",
        "axes[0].set_title('Original Phantom')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(torch.abs(adjoint_recon).cpu().numpy(), cmap='gray')\n",
        "axes[1].set_title('Adjoint (DAS-like) Recon')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(torch.abs(tv_recon_image).cpu().numpy(), cmap='gray')\n",
        "axes[2].set_title(f'TV Recon (lambda={lambda_tv_overall})')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}
