{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoDL Network Reconstruction Demo\n",
    "\n",
    "This notebook demonstrates setting up and using a Model-based Deep Learning (MoDL) network for MRI reconstruction using `reconlib`. We will focus on a 2D example for clarity and speed.\n",
    "\n",
    "The MoDL architecture alternates between a data consistency step (using the NUFFT operator) and a learned regularization step (a CNN denoiser)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Adjust path to import from reconlib (if not installed as a package)\n",
    "import sys\n",
    "import os\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..')))\n",
    "\n",
    "from reconlib.operators import NUFFTOperator\n",
    "from reconlib.deeplearning.models.resnet_denoiser import SimpleResNetDenoiser\n",
    "from reconlib.deeplearning.models.modl_network import MoDLNet\n",
    "from reconlib.deeplearning.datasets import MoDLDataset\n",
    "try:\n",
    "    from iternufft import generate_phantom_2d, generate_radial_trajectory_2d\n",
    "except ImportError:\n",
    "    print(\"WARN: iternufft.py not found or not in PYTHONPATH. Using dummy data generators.\")\n",
    "    def generate_phantom_2d(size, device='cpu'): return torch.rand((size,size), device=device) * 0.5\n",
    "    def generate_radial_trajectory_2d(num_spokes, samples_per_spoke, device='cpu'): \n",
    "        return (torch.rand((num_spokes*samples_per_spoke, 2), device=device) - 0.5) * np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Device and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2D Example Parameters\n",
    "image_size = 64 # Keep it small for a quick demo\n",
    "image_shape_2d = (image_size, image_size)\n",
    "dim = len(image_shape_2d)\n",
    "\n",
    "# K-space trajectory parameters (2D radial)\n",
    "k_traj_params_2d = {'num_spokes': 32, 'samples_per_spoke': image_size}\n",
    "\n",
    "# NUFFT Operator parameters\n",
    "oversamp_factor = tuple([2.0] * dim)\n",
    "kb_J = tuple([4] * dim)\n",
    "kb_alpha = tuple([2.34 * J for J in kb_J])\n",
    "Ld_table = tuple([2**8] * dim) # Table oversampling\n",
    "Kd_grid = tuple(int(N * os) for N, os in zip(image_shape_2d, oversamp_factor))\n",
    "\n",
    "nufft_op_params = {\n",
    "    'oversamp_factor': oversamp_factor,\n",
    "    'kb_J': kb_J,\n",
    "    'kb_alpha': kb_alpha,\n",
    "    'Ld': Ld_table,\n",
    "    'Kd': Kd_grid,\n",
    "    'kb_m': tuple([0.0]*dim),\n",
    "    'n_shift': tuple([0.0]*dim)\n",
    "}\n",
    "\n",
    "# MoDL Network parameters\n",
    "denoiser_channels = 1 # Working with magnitude images for simplicity in this demo\n",
    "denoiser_internal_channels = 32\n",
    "denoiser_num_blocks = 2\n",
    "modl_iterations = 3 # Number of unrolled iterations (K)\n",
    "lambda_dc_val = 0.05\n",
    "cg_iterations_dc = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we'll just use one sample from the dataset\n",
    "demo_dataset = MoDLDataset(\n",
    "    dataset_size=1, # Just one sample for demo\n",
    "    image_shape=image_shape_2d,\n",
    "    k_trajectory_func=generate_radial_trajectory_2d,\n",
    "    k_trajectory_params=k_traj_params_2d,\n",
    "    nufft_op_params=nufft_op_params,\n",
    "    phantom_func=generate_phantom_2d,\n",
    "    phantom_params={'size': image_size},\n",
    "    noise_level_kspace=0.01,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get the single sample\n",
    "x0_initial, y_observed, x_true = demo_dataset[0]\n",
    "\n",
    "print(f\"Initial reconstruction (x0) shape: {x0_initial.shape}\")\n",
    "print(f\"Observed k-space (y) shape: {y_observed.shape}\")\n",
    "print(f\"Ground truth image (x_true) shape: {x_true.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate NUFFTOperator, Denoiser, and MoDLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUFFT Operator (re-use the one from dataset for consistency or create new)\n",
    "nufft_op = demo_dataset.nufft_op \n",
    "\n",
    "# Denoiser CNN\n",
    "denoiser = SimpleResNetDenoiser(\n",
    "    in_channels=denoiser_channels, \n",
    "    out_channels=denoiser_channels,\n",
    "    num_internal_channels=denoiser_internal_channels,\n",
    "    num_blocks=denoiser_num_blocks\n",
    ").to(device)\n",
    "\n",
    "# MoDL Network\n",
    "modl_network = MoDLNet(\n",
    "    nufft_op=nufft_op,\n",
    "    denoiser_cnn=denoiser,\n",
    "    num_iterations=modl_iterations,\n",
    "    lambda_dc_initial=lambda_dc_val,\n",
    "    num_cg_iterations_dc=cg_iterations_dc\n",
    ").to(device)\n",
    "\n",
    "modl_network.eval() # Set to evaluation mode (as we are not training here)\n",
    "print(\"MoDLNet instantiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform Reconstruction (Inference)\n",
    "\n",
    "Since we don't have a pre-trained model in this simple demo, the reconstruction will use the randomly initialized weights of the denoiser. The purpose is to show the pipeline structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # MoDLNet expects initial_image_x0 of shape (*image_shape) and complex\n",
    "    # Our x0_initial is already complex and has the correct shape\n",
    "    # If denoiser_channels=1, MoDLNet's internal denoiser call needs to handle magnitude\n",
    "    # For this demo, we'll assume x0_initial (complex) is passed.\n",
    "    # The SimpleResNetDenoiser expects (N,C,H,W). MoDLNet's forward currently assumes single batch and handles unsqueezing.\n",
    "    \n",
    "    # If denoiser expects single channel (e.g. magnitude)\n",
    "    if denoiser_channels == 1:\n",
    "        print(\"Note: Using magnitude of x0 as input to MoDLNet due to denoiser_channels=1\")\n",
    "        # This is a simplification; typically complex data is fed through network parts\n",
    "        # and denoiser might operate on real/imag channels or magnitude then recombine.\n",
    "        # The current MoDLNet and SimpleResNetDenoiser setup might need adjustment for perfect complex handling.\n",
    "        # For this demo, we pass complex x0, and if denoiser is 1-channel, it will likely take abs() internally or error.\n",
    "        # Let's ensure the MoDLNet's denoiser call is robust or we adapt input here.    # For simplicity, we assume the MoDLNet's forward and internal denoiser call handle the channel logic.     # We pass the complex x0_initial that the DC block would use.\n",
    "        reconstructed_image = modl_network(y_observed, x0_initial) \n",
    "    else: # denoiser_channels == 2 (expects real/imag)\n",
    "        # The current MoDLNet and denoiser setup expects the denoiser_cnn to handle the input appropriately.\n",
    "        # If x0_initial is complex (H,W), SimpleResNetDenoiser expects (N,C,H,W). This is handled in SimpleResNetDenoiser's forward.\n",
    "        reconstructed_image = modl_network(y_observed, x0_initial)\n",
    "\n",
    "print(f\"Reconstructed image shape: {reconstructed_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axs[0].imshow(x_true.abs().cpu().numpy(), cmap='gray')\n",
    "axs[0].set_title('Ground Truth (x_true)')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(x0_initial.abs().cpu().numpy(), cmap='gray')\n",
    "axs[1].set_title('Initial Recon (A^H y)')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(reconstructed_image.abs().cpu().numpy(), cmap='gray')\n",
    "axs[2].set_title('MoDL Reconstructed (Untrained)')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adjointness Test (from previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Adjointness Test for 2D NUFFTOperator ---\")\n",
    "x_2d_test = torch.randn(image_shape_2d, dtype=torch.complex64, device=device)\n",
    "y_2d_test_shape = (nufft_op.k_trajectory.shape[0],)\n",
    "y_2d_test = torch.randn(y_2d_test_shape, dtype=torch.complex64, device=device)\n",
    "\n",
    "Ax_2d = nufft_op.op(x_2d_test)\n",
    "Aty_2d = nufft_op.op_adj(y_2d_test)\n",
    "\n",
    "lhs_2d = torch.sum(Ax_2d * torch.conj(y_2d_test))\n",
    "rhs_2d = torch.sum(x_2d_test * torch.conj(Aty_2d))\n",
    "\n",
    "print(f\"LHS (<Ax, y>): {lhs_2d.item()}\")\n",
    "print(f\"RHS (<x, A*y>): {rhs_2d.item()}\")\n",
    "abs_diff_2d = torch.abs(lhs_2d - rhs_2d).item()\n",
    "rel_diff_2d = abs_diff_2d / torch.abs(lhs_2d).item() if torch.abs(lhs_2d) > 1e-9 else 0.0\n",
    "print(f\"Absolute Difference: {abs_diff_2d:.6e}\")\n",
    "print(f\"Relative Difference: {rel_diff_2d:.6e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
